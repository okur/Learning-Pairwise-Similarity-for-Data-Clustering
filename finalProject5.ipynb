{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "naked-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, SpectralClustering\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hungarian-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset and seperate it to features and labels\n",
    "iris = datasets.load_iris()\n",
    "irisX = iris.data\n",
    "irisY = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sealed-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset and seperate it to features and labels\n",
    "breasC = datasets.load_breast_cancer()\n",
    "breasX = breasC.data\n",
    "breasY = breasC.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "standing-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optidigits dataset and seperate it to features and labels\n",
    "optidigits = datasets.load_digits()\n",
    "optidigitsX = optidigits.data\n",
    "optidigitsY = optidigits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "attached-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load yeast dataset and seperate it to features and labels\n",
    "yeastTxt = open(\"yeast.data\", \"r\")\n",
    "yeastLines = yeastTxt.readlines()\n",
    "firstData = []\n",
    "trainData = []\n",
    "lastData = []\n",
    "for line in yeastLines:\n",
    "    line = line.replace(\"\\n\", \"\")\n",
    "    split = line.split(\"  \")\n",
    "    firstData.append(split[0])\n",
    "    lastData.append(split[-1])\n",
    "    trainData.append([split[1], split[2], split[3], split[4], split[5], split[6], split[7], split[8]])\n",
    "    \n",
    "yeastFirst = np.asarray(firstData)\n",
    "yeastX = np.asarray(trainData)\n",
    "yeastLast = np.asarray(lastData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "gentle-affect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating coassociation matrix \n",
    "def create_coassociation_matrix(labels):\n",
    "    rows = []\n",
    "    cols = []\n",
    "    unique_labels = set(labels)\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(labels == label)[0]\n",
    "        for index1 in indices:\n",
    "            for index2 in indices:\n",
    "                rows.append(index1)\n",
    "                cols.append(index2)\n",
    "                \n",
    "    data = np.ones((len(rows),))\n",
    "    return csr_matrix((data, (rows, cols)), dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "suspected-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_similarity_mat(labels):\n",
    "    l_mat = np.repeat(np.asarray(labels).reshape(-1,1), len(labels), axis=1)\n",
    "    l_mat_t = l_mat.T\n",
    "\n",
    "    sim_mat = np.equal(l_mat, l_mat_t).astype(int)\n",
    "    return sim_mat\n",
    "\n",
    "\n",
    "def overall_quality(y,y_hat):\n",
    "    return np.mean(y == y_hat) * 100 \n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "# Class of Evidence Accumulation Cluster which has different kind of clustering algorithm\n",
    "# such as kmeans single link and average link methods\n",
    "class EAC(BaseEstimator, ClusterMixin):\n",
    "    def __init__(self, k, parameters_SC, cut_threshold=0.5):\n",
    "        self.cut_threshold = cut_threshold\n",
    "        self.k = k\n",
    "        self.parameters_SC = parameters_SC\n",
    "    def fit(self, X, y=None, method = None):\n",
    "        coassociation_algorithms = []\n",
    "        for i in self.k:\n",
    "            coassociation_algorithms.append(create_coassociation_matrix(self._kmeans_clustering(X,i)))\n",
    "            coassociation_algorithms.append(create_coassociation_matrix(self._singleLink_clustering(X,i)))\n",
    "        for parameters in self.parameters_SC:\n",
    "            coassociation_algorithms.append(create_coassociation_matrix(self._spectral_clustering(X, parameters)))\n",
    "        \n",
    "        C = sum(coassociation_algorithms)\n",
    "        \n",
    "        # get the average of the similarity mat\n",
    "        avgC = np.divide(C.toarray(), (len(self.k)*2 + len(self.parameters_SC)))\n",
    "\n",
    "        # flip the similarity. smaller value implies more similarity\n",
    "        avgC = np.abs(np.max(avgC) - avgC)\n",
    "\n",
    "        # build clusters\n",
    "        self.Z_ = linkage(avgC, method=\"single\")\n",
    "        self.labels_ = fcluster(self.Z_, min(self.k), criterion='inconsistent')\n",
    "        \n",
    "        \"\"\"\n",
    "        mst = minimum_spanning_tree(-C)\n",
    "        mst.data[mst.data > -self.cut_threshold] = 0\n",
    "        self.n_components, self.labels_ = connected_components(mst)\n",
    "        self.coassociation_matrix = coassociation_algorithms\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def _kmeans_clustering(self, X, k):\n",
    "        km = KMeans(n_clusters=k)\n",
    "        return km.fit_predict(X)\n",
    "    \n",
    "    def _averageLink_clustering(self, X, k):\n",
    "        alCluster = AgglomerativeClustering(n_clusters = k, linkage = \"average\")\n",
    "        alCluster.fit(X)\n",
    "        return alCluster.labels_\n",
    "    \n",
    "    def _singleLink_clustering(self, X, k):\n",
    "        slCluster = AgglomerativeClustering(n_clusters = k, linkage = \"single\")\n",
    "        slCluster.fit(X)\n",
    "        return slCluster.labels_\n",
    "    \n",
    "    def _spectral_clustering(self, X, parameters):\n",
    "        sc = SpectralClustering(n_clusters=parameters[0])\n",
    "        return sc.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "decent-antigua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.66666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bunya\\desktop\\grafteorisi\\graph\\lib\\site-packages\\ipykernel_launcher.py:39: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], dtype=int32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For iris dataset\n",
    "k = [3,5,10,12,15]\n",
    "parameters_SC = []\n",
    "eac1 = EAC(k, parameters_SC, cut_threshold=0.9)\n",
    "eac1.fit(irisX)\n",
    "print(overall_quality(irisY, (eac1.labels_ - 1)))\n",
    "eac1.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "supported-concept",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.66666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bunya\\desktop\\grafteorisi\\graph\\lib\\site-packages\\ipykernel_launcher.py:39: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For iris dataset\n",
    "k = [3,5,10,12,15]\n",
    "parameters_SC = [(3,0.1), (12,0.1)]\n",
    "eac2 = EAC(k, parameters_SC, cut_threshold=0.1)\n",
    "eac2.fit(irisX)\n",
    "print(overall_quality(irisY, (eac2.labels_ - 1)))\n",
    "(eac2.labels_ - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "orange-hypothesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.258347978910365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bunya\\desktop\\grafteorisi\\graph\\lib\\site-packages\\ipykernel_launcher.py:39: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For iris dataset\n",
    "k = [2, 3, 5, 10]\n",
    "parameters_SC = []\n",
    "eac3 = EAC(k, parameters_SC, cut_threshold = 0.5)\n",
    "eac3.fit(breasX)\n",
    "print(overall_quality(breasY, (eac3.labels_ - 1)))\n",
    "eac3.labels_\n",
    "breasY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "classical-broadway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eac3.labels_ - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Combining multiple clusterings using evidence accumulation (EAC).\n",
    "\"\"\"\n",
    "# Author: Yue Zhao <zhaoy@cmu.edu>\n",
    "# License: BSD 2 clause\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "from pyod.utils.utility import check_parameter\n",
    "\n",
    "from .base import BaseAggregator\n",
    "\n",
    "\n",
    "def _generate_similarity_mat(labels):\n",
    "    l_mat = np.repeat(labels, len(labels), axis=1)\n",
    "    l_mat_t = l_mat.T\n",
    "\n",
    "    sim_mat = np.equal(l_mat, l_mat_t).astype(int)\n",
    "    return sim_mat\n",
    "\n",
    "def overall_quality(y,y_hat):\n",
    "    return np.mean(y == y_hat) * 100 \n",
    "\n",
    "\n",
    "class EAC(BaseAggregator):\n",
    "\n",
    "    def __init__(self, base_estimators, n_clusters, linkage_method='single',\n",
    "                 weights=None, pre_fitted=False):\n",
    "\n",
    "        super(EAC, self).__init__(\n",
    "            base_estimators=base_estimators, pre_fitted=pre_fitted)\n",
    "\n",
    "        check_parameter(n_clusters, low=2, param_name='n_clusters')\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "        # set estimator weights\n",
    "        self._set_weights(weights)\n",
    "\n",
    "        self.linkage_method = linkage_method\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Validate inputs X\n",
    "        X = check_array(X)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # initialize similarity matrix\n",
    "        sim_mat_all = np.zeros([n_samples, n_samples])\n",
    "\n",
    "        if self.pre_fitted:\n",
    "            print(\"Training Skipped\")\n",
    "\n",
    "        else:\n",
    "            for clf in self.base_estimators:\n",
    "                clf.fit(X)\n",
    "                clf.fitted_ = True\n",
    "\n",
    "        for i, estimator in enumerate(self.base_estimators):\n",
    "            check_is_fitted(estimator, ['labels_'])\n",
    "\n",
    "            # get the labels from each base estimator\n",
    "            labels = estimator.labels_.reshape(n_samples, 1)\n",
    "\n",
    "            # generate the similarity matrix for the current estimator\n",
    "            sim_mat = _generate_similarity_mat(labels)\n",
    "\n",
    "            # add to the main similarity mat\n",
    "            sim_mat_all = sim_mat_all + sim_mat\n",
    "\n",
    "        # get the average of the similarity mat\n",
    "        sim_mat_avg = np.divide(sim_mat_all, self.n_base_estimators_)\n",
    "\n",
    "        # flip the similarity. smaller value implies more similarity\n",
    "        sim_mat_avg = np.abs(np.max(sim_mat_avg) - sim_mat_avg)\n",
    "\n",
    "        # build clusters\n",
    "        self.Z_ = linkage(sim_mat_avg, method=self.linkage_method)\n",
    "        self.labels_ = fcluster(self.Z_, self.n_clusters, criterion='maxclust')\n",
    "\n",
    "        # it may leads to different number of clusters as specified by the user\n",
    "        if len(np.unique(self.labels_)) != self.n_clusters:\n",
    "            warnings.warn(\n",
    "                'EAC generates {n} clusters instead of {n_clusters}'.format(\n",
    "                    n=len(np.unique(self.labels_)),\n",
    "                    n_clusters=self.n_clusters))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_predict(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.labels_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
